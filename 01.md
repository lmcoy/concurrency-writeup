# Concurrency, Parallelism, Asynchrony

## Concurrency & Parallelism

- Concurrency: 
    - composition of independently executing tasks (e.g. functions) 
    - two tasks can start, run, and complete in overlapping time periods
    - dealing with a lot of things at once
    - non-deterministic execution
        - multiple threads of execution, interleaved by a scheduler
        - result depends on  interleaving at runtime 
- Parallelism: 
    - simultanious execution of things
    - doing a lot of things at once (at same time instance)
    - deterministic execution, handles dependencies of sub computations/tasks

Concurrency & Parallelism are *not* the same but connected
- concurrent tasks might be executed in parallel
- when side effects (non-local state changes) occur, concurrency is needed for parallelism

### Notes about side effects
Think of side effects as something in a function that has observable effects on the outside 
besides returning a value. A simple example is a `print` function. Consider this function

```python
def f(x):
    print(x)
    return x
```

When you run it, you can see that it has run by looking at `stdout`.
If you look at `stdout` the effect of running this function is observable to you.
If you do not care about `stdout`, the printing is not observable to you and it is not a 
side effect for you.
Now imagine you start `f(1)` on thread 1 and `f(2)` on thread 2 in parallel. 
Depending on which thread prints first, the output will be
```
1
2
```
or 
```
2
1
```
If you consider the output to `stdout` as some kind of logging and you don't care about the order
everything is fine with your parallel execution (no side effect). 
It is deterministic because you ignore the non-deterministic effects. 

But as soon as you care about the order of what it prints to stdout, printing is a side effect and
you have to consider your program as non-deterministic. Hence, concurrency comes into play and
you have to introduce some kind of concurrency control.

Printing to `stdout` is just a simple example, the side effect can be anything (e.g. modifying a 
non-local variable, changing the DB,...)

## Examples for concurrency and parallelism

- not concurrent & not parallel
    - process all tasks sequentielly (one at a time)

- concurrent & not parallel:
    - executing multiple tasks at the same time 
    - no two task are executing at the same time instant
    - example: web browser
        mouse, keyboard, network, display,... running on a system with 1 CPU Core

- parallel & not concurrent
    - example: process multiple sub tasks on different CPU cores at the same time
        sum a large list of integers: 
            1. split list into two sub lists
            2. calculate the sums of the sub list on different CPU cores in parallel
            3. join the results by adding them
    - example: vector operations on CPU (SIMD,SSE): e.g. additions in parallel
       see [vector-operations.py](vector-operations.py) for an example in python

- parallel & concurrent
    - process multiple tasks concurrently on a multi core CPU


## Asynchrony

- start some work, do something else and get notified when work is finished
- decouples calling a function & getting the result
- useful for calls to external systems, e.g. DB...
- example: if you send an email, you don't freeze and wait for the response!
- blocking vs. non-blocking:
    blocking: send SQL query to DB and wait til you have the result and continue
    non-blocking: send SQL query to DB, do sth else, use DB result once available

### Example: REST API server

- the REST API server should deal with many user requests at the same time
    - we want concurrency
    - it can only deal with a few requests in parallel due to limited CPU cores
    - we do not want to block while waiting for the DB
        - blocking would mean *no* requests possible while we are waiting for DB
        - DB call should be async: 
            1. request that requires data from DB comes in
            2. ask DB for data
            3. do other task (e.g. other requests)
            4. respond once DB data is available

## Python

- Disclaimer: I'm not a python developer. I use it mainly for scripting.
- Use python for the following examples in order to look into it again & because it is expressive
- Use trio for examples
    - "Trio: a *friendly* Python library for async concurrency and I/O"
    - Docs in my opinion one of the best and friendliest introductions!
    - https://trio.readthedocs.io/en/stable/index.html
- global interpreter lock (GIL): interpreter allows exactly one thread to execute at a time
    - limited parallism (only in case of a blocking thread due to waiting for external computations the thread can release the lock)
    - trio uses concurrency

Structure:
- examples (usually python code) followed by explanation of concepts used in the 
   example.

## Example: Synchrony in Python

[sync-square.py](sync-square.py) contains a simple example of synchronous python code.

## Example: Basic Asynchrony in python

[async-square.py](async-square.py) contains a simple example of asynchrony in python using trio.

## Example: Forever

[forever.py](forever.py) contains an example that wraps an infinte loop into a async function.

## Scheduling

Scheduler is responsible when to run tasks.

### Preemptive

- the scheduler has control over tasks
- the scheduler can change which task is running
- a tasks is run for a time interval that the scheduler controls
    - a task can finish early and yield control to the scheduler
    - if a task does not finish within the time interval, the scheduler will interrupt the task, 
       and resume it later in a new time interval
- the scheduler can ensure fairness between tasks or give tasks a higher priority (= more runtime)
- for an operating system preemptive scheduling makes sense because an ill behaving
   task cannot block the whole system
- task interruption requires storing and restoring of the state of the task (context swichtes)
   https://en.wikipedia.org/wiki/Context_switch
- https://en.wikipedia.org/wiki/Preemption_(computing)

### Cooperative

- the scheduler has less control over tasks
    - a task has to yield control and if it does not it will run forever
- ill-behaved tasks can block the whole system because no other tasks can be assigned to run
- works well for concurrent tasks that work closely together
- https://en.wikipedia.org/wiki/Cooperative_multitasking

#### When to yield control

The `forever` function in [forever.py](forever.py) yields control after each
invocation of the async function. This gives control to the trio runtime as often as possible.
However, this can have performance implications because going back and forth will cost time
and you will mess up CPU caches etc. by switching. Hence, if the (infinite) loop is a 
performance critical loop, you might want to yield less often. This leads to less possibilities
for trio to run other tasks or timeouts. You have to find a compromise between performance
and time spacing of checkpoints where the computation can be cancelled e.g. due to 
timeouts.

## Example: trio nursery

[nursery.py](nursery.py)

## Blocking and non-blocking tasks
We use `trio.sleep()` in all examples as placeholder for some real asynchronous operation like
DB operations. `trio.sleep()` is non-blocking, i.e. control is yielded and trio can do something 
else. In our examples doing other sleeps. This is a good model for async operations like DB 
operations using a proper async library because our code will mostly wait until the DB server
computed and returned the result. 
You can see that we are not introducing any magic parallelism by replacing the non-blocking 
`trio.sleep()` with the blocking `time.sleep()`. trio runs the task that takes 5 seconds to 
complete and can start the next task only after the completion. Hence, the tasks are running
sequentially. The blocking `time.sleep` can be seen as a placeholder for external operations that
wait for the result or for compute intensive local operations that take a certain time to 
complete. When you try to run compute intensive tasks, trio will not speed up the execution with
concurrent execution. You are still limited by your one CPU. To speed up you computation you
have to employ parallelism (either with multiple operating system thread (beware the GIL), or
multiple processes, or even distribute the computation over multiple machines.)

## Nursery
Trio nurseries are a way to keep control over child tasks that you start. 
However, this is not the only way to handle concurrency. For instance the
goroutines of the go programming language are not managed this way. 
The nursery approach makes cancellations simple. In go you have to implement them
yourself (e.g. by a dedicated signal channel).
The go approach makes possible cancellations and their handling explicit while the trio
approach is more implicit and you need an understanding when cancellation can happen.
See the trio
[docs](https://trio.readthedocs.io/en/stable/reference-core.html#cancellation-and-timeouts) 
for information about cancellation in trio.

## Example: Composition

[composition.py](composition.py)

## Composition

Composition means that you build a larger system by composing (simple) building blocks.

### Function Composition
Functions are easily composable if calling the function does not imply side effects.

The simplest case are two (mathematical) functions that are composed the following way
```
add_1(x) = x + 1
sqrt(x) = ...

composite(x) = add_1(sqrt(x))
```
Since both functions `add_1` and `sqrt` only compute the result (no side effects), 
we can understand the function `composite` without looking into the implementation of 
`add_1` and `sqrt`. They just return numbers.
If both functions would have side effects (changing some state, e.g. print), we have to know
about the side effects when we analyze `composite`. Since the composed functions 
can contain again function calls, `composite` can become very complex.
Your computer will execute this perfectly but the human brain is not capable of keeping an
overview of everything that might be happening and unintended behaviour (i.e. bugs)
are likely.

Function calls that can be replaced with their result without changing the behaviour are called
`referentially transparent`(https://en.wikipedia.org/wiki/Referential_transparency). Such 
functions are composable easily in the above sense into bigger
programs. _Functionl Programming_ is about this function composition.

### Object Composition

Another view on composition is from a more object oriented view.
Objects are built by composing simpler objects with a "has a" relation.
For instance, in C
```c
struct birthday_t {
    int year;
    int month;
    int day;
};

struct person_t {
    char * name;
    struct birthday_t birthday;
};
```
defines a birthday. The birthday has a year, a month and a day. 
The example also defines a person that has a name and a birthday.
The fact that this example is in C shows that object composition is more fundamental than
object orientation but certainly a necessity for OO programming as in Java or C#.

Composition is such an important concept that in object oriented programming 
the design pattern `composition over inheritance` 
https://en.wikipedia.org/wiki/Composition_over_inheritance
is advocated. 
This means the defining principle of object oriented programming inheritance is discouraged if
composition is possible.
Languages like "go" even abandon inheritance and allow only composition. (https://golang.org/doc/faq#Is_Go_an_object-oriented_language  https://talks.golang.org/2012/chat.slide#5)

### Composition in trio

The above sections show that composition is an important technique to build larger systems.
[composition.py](composition.py) shows how trio tasks compose.
We just reuse the `child` and `parent` functions from the previous example and implement
the new `delay` and `withHeartbeat` functions without looking into the details of parent. 
Finally, we can combine both in a functional composition way. Beware side effects, e.g.
changes of shared variables. We will see later how channels can be used to avoid shared
state in concurrent trio tasks.
(strictly speaking printing could be considered as side effect (s. above))

## Example: Retry

[retry.py](retry.py)

## Retry

When we deal with distributed systems, we have to expect failures when we communicate with
other systems (e.g. the other system could be down, there could be network hick ups, etc.).

To be resilient in case of failure, an important pattern is to retry calls to the external systems
in case of failures.

If a call fails, it makes sense to retry the call after a short while. However, if also the first
retry fails, it does not make sense to retry immediately again because the target seems 
to be not reachable at the moment. Hence, it makes sense to wait before we retry.
An often used approach is to use an `exponential backoff` where the time between retries
grows exponentially (often powers of 2 are used: 1s, 2s, 4s, 8s, ...).

Suppose that multiple clients experience an issue at the same time because the service 
is overloaded (e.g. HTTP 503 or 504 error codes).
If all clients use an exponential backoff, they would retry at the same time and the problem
would not be resolved by retrying. To avoid this problem, randomness is introduced to the 
waiting time.
This technique is called `exponential backoff with jitter`.

## Example: Racing

[racing.py](racing.py)

The racing examples considers a case where multiple services can be used to get a result.
To maximize performance requests are sent to all services and we take the first returned results
and cancel all losing tasks. In our example the task just sleeps. Hence, no canceling action has 
to be done except canceling the task in trio.

This examples shows two important features of trio

- we can start many concurrent tasks
- task synchronization with channels

They are described in the following in more detail

## Many concurrent tasks

As shown in the example many concurrent tasks can be used in trio (and other 
concurrent languages and frameworks (e.g. go)). This is possible because the
threading happens within the library and not on the level of operating system level 
threads.

Operating system level threads:
- managed by OS (creation, scheduling, etc.)
- when a new thread is started, the OS starts a new thread, scheduling will be preemptive 

User level threads:
- trio tasks (coroutine), goroutines in golang, fibers, green threads...
- managed by the runtime (trio, go, ...)
- many user level threads run on one operating system thread
- depending on the runtime the scheduling can be cooperative or preemptive

Often thread pools are used to run user level threads on multiple operating system threads.
Examples of thread pools:
- bounded: limited number of threads
- unbounded: unlimited number of threads.
    - use cached (=reuse threads) + keep alive (=remove threads that idle for too long)
    - can be useful for blocking tasks 
       https://gist.github.com/djspiewak/46b543800958cf61af6efa8e072bfd5c 


## Task synchronization with channels

**TODO**

## Example: par

[par.py](par.py)

## Example: prime

prime is a prime sieve and one of the examples in the golang playground.

- port to trio: [prime.py](prime.py)
- original golang: [prime.go](prime.go)
- port to scala using zio: [prime.scala](prime.scala) 

This shows that concurrency can be used to design & implement algorithms.


## Communication between concurrent tasks (general)

- Future/Promise/async/await
    - model results that will be available in the future & their dependencies

- message passing (MPI, OpenMPI)
    - channels (trio, go,...)
    - actor model (akka, elixir,...)
    - ...
    
- shared memory
    - mutex
    - semaphore
    - ...


## Sources

- Communicating Sequential Processes - Hoare, C.A.R. (1978)
https://dl.acm.org/doi/pdf/10.1145/359576.359585

- trio documentation (excellent introduction of concepts also valuable for non-python developers)
https://trio.readthedocs.io/en/stable/index.html

- Cooperative vs. Preemptive Scheduling: excellent description with comparison of different approaches/languages
https://medium.com/traveloka-engineering/3b10c5a920fe

- Rob Pike - Concurrency Is Not Parallelism
https://www.youtube.com/watch?v=cN_DpYBzKso

- Adam Warski - Will Project Loom obliterate Java Futures?
https://blog.softwaremill.com/fb1a28508232

- https://en.wikipedia.org/wiki/Parallel_computing

- https://en.wikipedia.org/wiki/Concurrent_computing

- Parallelism /= Concurrency
https://ghcmutterings.wordpress.com/2009/10/06/parallelism-concurrency/

- Parallelism Is Not Concurrency
https://existentialtype.wordpress.com/2011/03/17/parallelism-is-not-concurrency/